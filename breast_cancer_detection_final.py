# -*- coding: utf-8 -*-
"""Breast Cancer Detection_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dqs29_HWFGz7CISn3DnAHfzL1AmKf7Bi
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

"""Importing the dataset"""

dataset = pd.read_csv('/content/drive/MyDrive/Avanti/Avanti/data.csv')

dataset.head()

"""Feature engineering"""

dataset.drop('id', axis = 1,inplace=True)

#dataset = dataset.drop(dataset.iloc[:,10:20],axis =1)

dataset

print("Breast Cancer data set dimensions : {}".format(dataset.shape))

X = dataset.iloc[:, 1:30].values
y = dataset.iloc[:, 30].values

print(X)

print(y)

"""Checking for missing values"""

dataset.isnull().sum()
dataset.isna().sum()

dataset.duplicated().sum()

dataset.info()

"""Encoding dependent variable (Benign:0, Malign:1)"""

y = dataset['diagnosis'].copy()

y = y.map({'B':0,'M':1})
y

y

dataset.describe()

"""Data Visualisation"""

import seaborn as sns

sns.countplot(dataset['diagnosis'])
plt.title('Count of cancer type')
plt.xlabel('Cancer lethality')
plt.ylabel('Count')
plt.show()

sns.set(style="ticks")
sns.pairplot(dataset, palette = ('r', 'b'), hue= "diagnosis")
plt.show()

dataset.corr()

plt.imshow(dataset.corr(), cmap= plt.cm.Blues, interpolation= 'nearest')
tick_marks = [i for i in range(len(dataset.iloc[1:10]))]
plt.xticks(tick_marks, dataset.iloc[:,1:10], rotation= 'vertical')
plt.yticks(tick_marks, dataset.iloc[:,1:10])
plt.show()

plt.figure(figsize = (20,20))

corr = dataset.corr()
mask = np.triu(np.ones_like(corr, dtype = bool))

sns.heatmap(corr, mask = mask, linewidths = 1, annot = True, fmt = ".2f")
plt.show()

plt.figure(figsize = (20, 15))
plotnumber = 1

for column in dataset:
    if plotnumber <= 30:
        ax = plt.subplot(5, 6, plotnumber)
        sns.distplot(dataset[column])
        plt.xlabel(column)
        
    plotnumber += 1

plt.tight_layout()
plt.show()

sns.boxplot(x='texture_mean',data= dataset , hue= "diagnosis" ,width= 0.4)

radius_mean = dataset["radius_mean"]
texture_mean = dataset['texture_mean']
perimeter_mean = dataset['perimeter_mean']
area_mean = dataset['area_mean']
smoothness_mean = dataset["smoothness_mean"]
compactness_mean = dataset['compactness_mean']
concavity_mean = dataset['concavity_mean']
#concave points_mean = dataset['concave points_mean']
symmetry_mean = dataset['symmetry_mean']
fractal_dimension_mean = dataset['fractal_dimension_mean']

columns = [radius_mean, texture_mean ,perimeter_mean ,area_mean ,smoothness_mean ,compactness_mean ,concavity_mean ,symmetry_mean ,fractal_dimension_mean]

fig, ax = plt.subplots( )
ax.boxplot(columns)
plt.show()



"""Outlier resolving"""

def mod_outlier(dataset):
        dataset1 = dataset.copy()
        dataset = dataset._get_numeric_data()

        q1 = dataset.quantile(0.25)
        q3 = dataset.quantile(0.75)

        iqr = q3 - q1

        lower_bound = q1 -(1.5 * iqr) 
        upper_bound = q3 +(1.5 * iqr)

        for col in dataset.columns:
            for i in range(0,len(dataset[col])):
                if dataset[col][i] < lower_bound[col]:            
                    dataset[col][i] = lower_bound[col]

                if dataset[col][i] > upper_bound[col]:            
                    dataset[col][i] = upper_bound[col]    

        for col in dataset.columns:
            dataset1[col] = dataset[col]

        return(dataset1)

dataset = mod_outlier(dataset)

radius_mean = dataset["radius_mean"]
texture_mean = dataset['texture_mean']
perimeter_mean = dataset['perimeter_mean']
area_mean = dataset['area_mean']
smoothness_mean = dataset["smoothness_mean"]
compactness_mean = dataset['compactness_mean']
concavity_mean = dataset['concavity_mean']
#concave points_mean = dataset['concave points_mean']
symmetry_mean = dataset['symmetry_mean']
fractal_dimension_mean = dataset['fractal_dimension_mean']

columns = [radius_mean, texture_mean ,perimeter_mean ,area_mean ,smoothness_mean ,compactness_mean ,concavity_mean ,symmetry_mean ,fractal_dimension_mean]

fig, ax = plt.subplots( )
ax.boxplot(columns)
plt.show()

print((dataset.shape))

dataset.head()

dataset.drop("Unnamed: 0",axis=1)

#dataset.drop('area_mean', axis = 1,inplace=True)

#dataset.drop('area_worst', axis = 1,inplace=True)

X = dataset.iloc[:, 0:30].values
y = dataset.iloc[:, 30].values

X =pd.DataFrame(X)

y = dataset['diagnosis'].copy()

y = y.map({'B':0,'M':1})
y

print(X)

y

print("The Number of Samples in the dataset: ", len(dataset))
print('Class 0 :', round(dataset['diagnosis'].value_counts()[0]
                      /len(dataset) * 100, 2), '% of the dataset')
   
print('Class 1 :', round(dataset['diagnosis'].value_counts()[1]
                      /len(dataset) * 100, 2), '% of the dataset')

dataset['diagnosis'].value_counts().plot(kind='bar')
print(dataset['diagnosis'].value_counts())
plt.show()

"""Oversampling using SMOTE"""

from imblearn.over_sampling import SMOTE

X.shape

y.shape

y

sm = SMOTE(random_state = 42)
   
X_res, y_res = sm.fit_resample(X,y)
   
X_res = pd.DataFrame(X_res)
Y_res = pd.DataFrame(y_res)
   
   
print("After SMOTE Over Sampling Of Minor Class Total Samples are :", len(Y_res))
print('Class 0 :', round(Y_res[0].value_counts()[0]
                 /len(Y_res) * 100, 2), '% of the dataset')
   
print('Class 1 :', round(Y_res[0].value_counts()[1]
                 /len(Y_res) * 100, 2), '% of the dataset')

print(X_res)

X_res.shape

Y_res

Y_res.value_counts().plot(kind='bar')
print(Y_res.value_counts())
plt.show()

"""Splitting the dataset into the Training set and Test set"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_res, Y_res, test_size = 0.3, random_state = 0)

X_test

#ds=pd.DataFrame(X_test)
#pc_train = pd.DataFrame(pca.transform(X_train), columns=["PC" + str(i + 1) for i in range(n_components)])

X_test.columns = ["radius_mean",	"texture_mean",	"perimeter_mean",	"area_mean", "smoothness_mean",	"compactness_mean",	"concavity_mean",
                       "concave points_mean",	"symmetry_mean",	"fractal_dimension_mean", 'radius_se','texture_se',  'perimeter_se', 'area_se',       
                        'smoothness_se', 'compactness_se', 'concavity_se','concave points_se','symmetry_se','fractal_dimension_se',  "radius_worst",	"texture_worst",
                   "perimeter_worst",	"area_worst","smoothness_worst",	"compactness_worst",	"concavity_worst",	"concave points_worst",	"symmetry_worst",	"fractal_dimension_worst"]

X_test.to_csv('test.csv')

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)

X_train = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)
X_test = pd.DataFrame(scaler.transform(X_test), columns=X_train.columns)

X_train

X_test

print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)

"""PCA"""

from sklearn.decomposition import PCA
n_components = 5

pca = PCA(n_components=n_components)
pca.fit(X_train)

pc_train = pd.DataFrame(pca.transform(X_train), columns=["PC" + str(i + 1) for i in range(n_components)])
pc_test = pd.DataFrame(pca.transform(X_test), columns=["PC" + str(i + 1) for i in range(n_components)])

pc_train

pca.explained_variance_ratio_

pca.explained_variance_ratio_.cumsum()

plt.figure(figsize=(16, 10))
sns.barplot(x=pca.explained_variance_ratio_, y=["PC" + str(i + 1) for i in range(n_components)], orient='h', palette='husl')
plt.xlim(0., 1.)
plt.xlabel("Proportion of Variance in Original Data")
plt.title("Principal Component Variance")
plt.show()



"""Training

Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score 
# On the original data
original_model = LogisticRegression()
original_model.fit(X_train, y_train)

print("Model Accuracy (Original Data): {:.5f}%".format(original_model.score(X_test, y_test) * 100))

y_pred = original_model.predict(X_test)
y_pred

original_model_acc = accuracy_score(y_test, original_model.predict(X_test))
print(original_model_acc)

print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# On the principal components
pca_model = LogisticRegression()
pca_model.fit(pc_train, y_train)

print("Model Accuracy (PCA Data): {:.5f}%".format(pca_model.score(pc_test, y_test) * 100))

y_pred_pca = pca_model.predict(pc_test)
y_pred_pca

pca_model_acc = accuracy_score(y_test, pca_model.predict(pc_test))
print(pca_model_acc)

print(confusion_matrix(y_test, y_pred_pca))
print(classification_report(y_test, y_pred_pca))

"""KNN"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier()
knn.fit(pc_train, y_train)

y_pred_knn = knn.predict(pc_test)
y_pred_knn

print(accuracy_score(y_train, knn.predict(pc_train)))
knn_acc = accuracy_score(y_test, knn.predict(pc_test))
print(knn_acc)

print(confusion_matrix(y_test, y_pred_knn))
print(classification_report(y_test, y_pred_knn))

"""Support Vector Classifier"""

#with principal components
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

svc = SVC()
parameters = {
    'gamma' : [0.0001, 0.001, 0.01, 0.1],
    'C' : [0.01, 0.05, 0.5, 0.1, 1, 10, 15, 20]
}

grid_search = GridSearchCV(svc, parameters)
grid_search.fit(pc_train, y_train)

grid_search.best_params_
grid_search.best_score_

svc = SVC(C = 10, gamma = 0.01)
svc.fit(pc_train, y_train)

y_pred_pca_svc = svc.predict(pc_test)
y_pred_pca_svc

print(accuracy_score(y_train, svc.predict(pc_train)))

svc_acc = accuracy_score(y_test, svc.predict(pc_test))
print(svc_acc)

print(confusion_matrix(y_test, y_pred_pca_svc))
print(classification_report(y_test, y_pred_pca_svc))

"""
Random Forest Classifier"""

#with principal components
from sklearn.ensemble import RandomForestClassifier

rand_clf = RandomForestClassifier(criterion = 'entropy', max_depth = 11, max_features = 'auto',
                                  min_samples_leaf = 2, min_samples_split = 3, n_estimators = 100)
rand_clf.fit(pc_train, y_train)

y_pred_pca_rf = rand_clf.predict(pc_test)
y_pred_pca_rf

print(accuracy_score(y_train, rand_clf.predict(pc_train)))

ran_clf_acc = accuracy_score(y_test, rand_clf.predict(pc_test))
print(ran_clf_acc)

print(confusion_matrix(y_test, y_pred_pca_rf))
print(classification_report(y_test, y_pred_pca_rf))

"""Model Score Comparison """

models = pd.DataFrame({
    'Model': ['pca_model', 'KNN', 'SVC', 'Random Forest Classifier'],
    'Score': [pca_model_acc, knn_acc, svc_acc, ran_clf_acc]
})

models.sort_values(by = 'Score', ascending = False)

import pickle

filename= "model.pkl"
#exporting random forest model
model_pkl= open(filename, 'wb')
data = pickle.dump(rand_clf, model_pkl)

model_pkl.close()

#exporting pca file(pca.pkl)
pca = pickle.dump(pca, open("pca.pkl","wb"))

#exporting standard scaler file
pickle.dump(scaler, open('scaler.pkl','wb'))

#scaler = pickle.load(open('scaler.pkl','rb'))
